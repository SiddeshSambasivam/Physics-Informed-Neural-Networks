{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9e554901",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch \n",
    "import scipy.io\n",
    "import numpy as np\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm import trange\n",
    "\n",
    "from pylab import meshgrid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83e1021",
   "metadata": {},
   "source": [
    "**Two functions**\n",
    "1. $f(x,t)$ -> function to calculate the residual of the NN output\n",
    "2. $u(x,t)$ -> NN\n",
    "\n",
    "* $n_u$ is intial and boundary points\n",
    "    * sample -> (t, x)\n",
    "    * if sample[0] in (0, 1) or sample[1] in (-1,1): add to BC_IC_pts\n",
    "    * else: add to collocation points\n",
    "    \n",
    "    \n",
    "    \n",
    "* $n_f$ is collocation points\n",
    "\n",
    "\n",
    "* $n_u$ = 100 and $n_f$ = 10,000\n",
    "\n",
    "* With 8 Hidden layers with each 40 neurons, gives an $L_2$ error of $5.6e−04$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "660594f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((256, 1), (100, 1), (256, 100))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_PATH = \"../Data/burgers_shock.mat\"\n",
    "data_dict = scipy.io.loadmat(DATA_PATH)\n",
    "\n",
    "n_u, n_f = 100, 10000\n",
    "\n",
    "x_data,t_data, u_data = data_dict['x'], data_dict['t'], data_dict['usol']\n",
    "x_data.shape, t_data.shape, u_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "41e1bb0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((256, 100), (256, 100))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u_t, u_x = meshgrid(t_data, x_data)\n",
    "u_t.shape, u_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9a5c0e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "u_data_transformed = u_data.flatten()[:, None] # (25600,1)\n",
    "training_points = np.hstack((u_t.flatten()[:, None], u_x.flatten()[:, None])) # (25600, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7d355a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "IC_X, IC_Y = list(), list() # Initial and boundary points\n",
    "CC_X, CC_Y = list(), list() # Collocation points\n",
    "\n",
    "for idx, sample in enumerate(training_points):\n",
    "    \n",
    "    t, x = sample\n",
    "    if t in [0,1] or x in [-1,1]:\n",
    "        IC_X.append(sample)\n",
    "        IC_Y.append(u_data_transformed[idx])\n",
    "    else:\n",
    "        CC_X.append(sample)\n",
    "        CC_Y.append(u_data_transformed[idx])\n",
    "        \n",
    "IC_X = np.array(IC_X)\n",
    "IC_Y = np.array(IC_Y)\n",
    "\n",
    "CC_X = np.array(CC_X)\n",
    "CC_Y = np.array(CC_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8ee810e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_u_idx = list(np.random.choice(len(IC_X), n_u))\n",
    "n_f_idx = list(np.random.choice(len(CC_X), n_f))\n",
    "\n",
    "u_x = torch.tensor(IC_X[n_u_idx, 1:2], requires_grad=True).float()\n",
    "u_t = torch.tensor(IC_X[n_u_idx, 0:1], requires_grad=True).float()\n",
    "u_u = torch.tensor(IC_Y[n_u_idx, :], requires_grad=True).float()\n",
    "\n",
    "f_x = torch.tensor(CC_X[n_f_idx, 1:2], requires_grad=True).float()\n",
    "f_t = torch.tensor(CC_X[n_f_idx, 0:1], requires_grad=True).float()\n",
    "f_u = torch.tensor(CC_Y[n_f_idx, :], requires_grad=True).float()\n",
    "\n",
    "train_x = torch.cat((u_x, f_x), dim=0)\n",
    "train_t = torch.cat((u_t, f_t), dim=0)\n",
    "train_u = torch.cat((u_u, f_u), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "a001e7cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.parameter.Parameter'> torch.Size([40, 2])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([40])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([40, 40])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([40])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([40, 40])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([40])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([1, 40])\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "class PhysicsINN(nn.Module):\n",
    "    \n",
    "    '''\n",
    "    Physics Informed Neural Network\n",
    "    Written: Siddesh Sambasivam Suseela\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    def __init__(self, num_layers:int=2, num_neurons:int=20) -> None:\n",
    "        \n",
    "        super(PhysicsINN, self).__init__()\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        self.num_neurons = num_neurons\n",
    "        \n",
    "        # Each hidden layer contained 20 neurons and a hyperbolic tangent activation function.\n",
    "        self.activation_func = torch.nn.Tanh\n",
    "        \n",
    "        ordered_layers = list()\n",
    "                \n",
    "        ordered_layers.append((\"input_layer\", nn.Linear(2, self.num_neurons)))\n",
    "        ordered_layers.append((\"input_activation\", self.activation_func()))\n",
    "        \n",
    "        # Create num_layers-2 linear layers with num_neuron neurons and tanh activation function\n",
    "        for i in range(self.num_layers-2):\n",
    "            \n",
    "            ordered_layers.append((\"layer_%d\" % (i+1), nn.Linear(self.num_neurons, self.num_neurons)))\n",
    "            ordered_layers.append((\"layer_%d_activation\" % (i+1), self.activation_func()))\n",
    "                        \n",
    "        ordered_layers.append((\"output_layer\", nn.Linear(self.num_neurons, 1)))\n",
    "        \n",
    "        self.net = nn.Sequential(OrderedDict(ordered_layers))    \n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self, ) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the weights and biases of all the layers in the model\n",
    "        \n",
    "        NOTE: According to the paper, the model's weights are initialized by xaviers' distribution\n",
    "        and biases are initialized as zeros\n",
    "        \n",
    "        \"\"\"\n",
    "        for param in self.parameters():\n",
    "            if len(param.shape) >= 2: torch.nn.init.xavier_normal_(param)\n",
    "            elif len(param.shape) == 1: torch.nn.init.zeros_(param)\n",
    "\n",
    "        \n",
    "    def forward(self, inputs) -> torch.Tensor:\n",
    "        '''returns the output from the model'''\n",
    "        \n",
    "        out = self.net(inputs)\n",
    "\n",
    "        return out \n",
    "    \n",
    "model = PhysicsINN(4, 40)\n",
    "\n",
    "# Check the layer sizes\n",
    "for param in model.parameters():\n",
    "    print(type(param), param.size())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "f68bca78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.00049847963964566588: 100%|██████████| 100/100 [01:53<00:00,  1.14s/it]\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 100\n",
    "optimizer = torch.optim.LBFGS(model.parameters())\n",
    "\n",
    "t_bar = trange(EPOCHS)\n",
    "\n",
    "for epoch in t_bar:\n",
    "    \n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(torch.cat((train_t, train_x), dim=1))\n",
    "        \n",
    "        u_grad_x =  torch.autograd.grad(output, train_x, retain_graph=True, create_graph=True, grad_outputs=torch.ones_like(output),allow_unused=True)[0]\n",
    "        u_grad_xx = torch.autograd.grad(u_grad_x, train_x, retain_graph=True, create_graph=True, grad_outputs=torch.ones_like(output),allow_unused=True)[0]\n",
    "        \n",
    "        u_grad_t = torch.autograd.grad(output, train_t, retain_graph=True, create_graph=True, grad_outputs=torch.ones_like(output),allow_unused=True)[0]\n",
    "        \n",
    "        f = u_grad_t + output*u_grad_x - (0.01/np.pi) * u_grad_xx\n",
    "        \n",
    "        mse_f = torch.mean(torch.square(f))\n",
    "        mse_u = torch.mean(torch.square(output - train_u))\n",
    "        \n",
    "        loss = mse_f + mse_u\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        t_bar.set_description(\"loss: %.20f\" % loss.item())\n",
    "        t_bar.refresh() # to show immediately the update\n",
    "    \n",
    "        return loss\n",
    "    \n",
    "    optimizer.step(closure)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2169f16b",
   "metadata": {},
   "source": [
    "$4.9e-4$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430b51d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
